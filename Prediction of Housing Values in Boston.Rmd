---
title: "Prediction of Housing Values in Boston"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

**File name:** "housing.proper.csv" (given in the problem set) has housing values in the suburbs of Boston  

- **506  observations** and **13 predictor variables** (independent variables or X or features)      
- Median price is a more accurate measure of market prices as it is not affected by outliers       

```{R}

library (tidyverse)  
library (ggplot2)  
library(corrplot)
library(MASS)
library(dplyr)
library(broom)

housing_data = read.csv("housing.proper.csv") #loading the dataset
housing_data_df= data.frame(housing_data)
colnames(housing_data_df) <- c("X1","X2","X3","X4","X5","X6","X7","X8",
                            "X9","X10","X11","X12","X13", "Y") #transforming column names
lnY = log(housing_data_df$Y)
```
# Exploratory Data Analysis    

**Objective of EDA** : To choose important variables or features of this dataset
Visualisations are used the explain relationship between features and target variable    


**Split the data into 70% training set (model building data set) and 30% testing set**


```{r}
set.seed(1234) #set seed to ensure same random numbers are generated
require(caTools)
sample = sample.split(housing_data_df, SplitRatio = 0.7)
#Splits the dataset "housing_data" in the ratio mentioned. Marks rows as logical true and remaining as logical false.  

training_housing_data = subset(housing_data_df, sample == TRUE)  #creates training dataset with rows marked true
print 
lny = log(training_housing_data$Y)
testing_housing_data = subset(housing_data_df, sample ==FALSE)
lny_testing = log(testing_housing_data)
```
**Choosing features through Summary Statistics, Correlation Plot, Heat Map and Boxplot**  

EDA is performed on the complete dataset

```{r}

summary (housing_data_df) # to get statistic of each variable to detect outliers  

housing_data.cor = round(cor(housing_data_df), 3) #correlation between features and median housing values
housing_data.cor


heatmap(housing_data.cor, Rowv = NA, Colv = NA)

corrplot(cor(housing_data_df), method = "number", type = "upper", diag = FALSE)
```

**Removing Charles River Dummy Variable (X4)** as the Correlation between Median House Value and X4 is low (0.18). Smaller value of correlation signifies that the presence of Charles River has little to no impact on the value median value of houses.   

Strong positive correlation (0.91) between X9 (index of accessibility to radial highways) and X10 (full-value property-tax rate per 10,000) suggests Multicollinearity, which means we can drop one variable with a lower correlation with Y. Here, higher accessibility to radial highways is connected to a high tax value of a property. **I will drop X9 (index of accessibility to radial highways)** as it has a lower correlation with median housing values as compared to X10 (full-value property-tax rate per 10,000). **X9 is a repetitive predictor.**   



Strong Negative correlation (-0.75) between X7 (proportion of owner occupied units built prior to 1940) and X8 (weighted distances to five Boston employment centres)



```{r}
housing_data.box = boxplot(housing_data_df, xlab= "Features", ylab ="Frequency") #boxplot to detect outliers 

```
There are outliers in **X1(per capita crime rate by town)**, **X2(proportion of residential land zoned for lots over 25,000 sq.ft.)** and **X12 (proportion of African Americans by town)**   

There is high variability in **X10 (full-value property-tax rate per 10,000)**  

Plotting the model generates 4 plots: Normal QQ plot. Resdiuals vs Fitted. Scale vs Location. Residuals vs Leverage.


```{r}
full_housing_model =lm(Y~X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 ,data=housing_data_df)
log_full_housing_model =lm(lnY~X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X11 + X12 + X13 ,data=housing_data_df)


summary(full_housing_model)
tidy(full_housing_model) %>% arrange(p.value)
plot(log_full_housing_model)
pairs(housing_data_df)



#We have already removed X9 due to multicollinearity with X10, another reason to remove it is it's *high p-value*. 
#We will remove X8 too as it has the highest p-value as compared to all the other variables. High p-value suggests changes in X8 are not associated with changes in Y (median housing values)


```
We remove every variable with a pvalue greater than 0.05.   
Through EDA we have removed 4 variables (X3, X4, X7, X9, X12)    
Since X13 has the smallest p-value it becomes the first step for Forward Stepwise Selection   

# Model Selection and Validation

Fitting the model with selected variables

```{r}
log_full_housing_model =lm(lny~X1 + X2 + X5 + X6 + X8 + X10 + X11 + X13 ,data=training_housing_data)
s <- summary(log_full_housing_model)$sigma
# Selection criteria for the full model
SSres <- sum(log_full_housing_model$residuals^2)
Rsq <- summary(log_full_housing_model$r.squared)
Rsq_adj <- summary(log_full_housing_model)$adj.r.squared
p_prime <- length(log_full_housing_model$coefficients)
n <- nrow(training_housing_data)
C <- SSres/s^2 + 2*p_prime - n
AIC <- n*log(SSres) - n*log(n) + 2*p_prime


# Function that returns the selection criteria
select_criteria = function(model, n, s)
{
  SSres <- sum(model$residuals^2)
  Rsq <- summary(model)$r.squared
  Rsq_adj <- summary(model)$adj.r.squared
  p_prime <- length(model$coefficients)
  C <- SSres/s^2 + 2*p_prime - n
  AIC <- n*log(SSres) - n*log(n) + 2*p_prime
  res <- c(SSres, Rsq, Rsq_adj, C, AIC)
  names(res) <- c("SSres", "Rsq", "Rsq_adj", "C", "AIC")
  return(res)
}



rbind(
  select_criteria(lm(lny~X1+X2, data = training_housing_data), n, s),
  select_criteria(lm(lny~X1+X2+X6, data = training_housing_data), n, s),
  select_criteria(lm(lny~X1+X2+X3+X6+X8, data = training_housing_data), n, s),
  select_criteria(lm(lny~X1+X2+X5+X6+X8, data = training_housing_data), n, s),
  select_criteria(lm(lny~X1+X2+X3+X6+X9, data = training_housing_data), n, s),
  select_criteria(lm(lny~X1+X2+X5+X6+X8+X10, data = training_housing_data), n, s),
  select_criteria(lm(lny~X1+X2+X5+X6+X8+X10+X11, data = training_housing_data), n, s),
  select_criteria(lm(lny~X1+X2+X5+X6+X8+X10+X11+X13, data = training_housing_data), n, s),
  select_criteria(lm(lny~X1+X2+X3+X5+X6+X8+X10+X11+X13, data = training_housing_data), n, s))

sum((lny) - mean(lny)^2)
```
Model lny~X1+X2+X5+X6+X8+X10+X11+X13 has     
  - **Coefficient of Determination 0.777 and Adjusted coefficient of determination is 0.771**. Adding more variables in the next model causes a very minute change in both. Therefore, the current model is correct   
  - the smallest **Akaike’s Information Criterion, (AIC: -1073)**    
  - **Mallows’ Cp** is close to p'and is equal to **9**
  

```
We validate the reduced model against the boston housing testing set and find that **Mean squared prediction error** (MSPE)  & MSRes are quite closer together indicating a good predictive ability of the model.

#  Model Validation  








  
 
  
